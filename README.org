;-*- mode: Org; fill-column: 110;-*-

slack: https://app.slack.com/client/T01ATQK62F8/C02R98X7DS9

homework: https://courses.datatalks.club/mlops-zoomcamp-2024/
- https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/cohorts/2024/01-intro/homework.md

course: https://github.com/DataTalksClub/mlops-zoomcamp/tree/main/01-intro

book https://github.com/alexeygrigorev/mlbookcamp-code
* intro

** 1) Download the data for January and February 2023. Read the data for January. How many columns are there?
https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page

requirements.txt:
- pandas==2.2.2
- pyarrow==16.1.0
- fastparquet==2024.2.0

: import pandas as pd
: df = pd.read_parquet("yellow_tripdata_2023-01.parquet")
: print(len(df.columns))
19

** 2) Computing duration

Now let's compute the duration variable. It should contain the duration of a ride in minutes.
What's the standard deviation of the trips duration in January?

#+begin_src python :tangle /tmp/out.py :results none :exports code :eval no
df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)
df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)

df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime
df.duration = df.duration.apply(lambda td:td.total_seconds() /60)
#+end_src

42.59
** 3) Dropping outliers
Next, we need to check the distribution of the duration variable. There are some outliers. Let's remove them and keep only the records where the duration was between 1 and 60 minutes (inclusive).
What fraction of the records left after you dropped the outliers?

#+begin_src python :tangle /tmp/out.py :results none :exports code :eval no
print(df.shape())
df = df[(df.duration >=1) & (df.duration <=60)]
print(df.shape())
#+end_src
3009173/3066766 = 0.98122

98%


** 4) One-hot encoding
Let's apply one-hot encoding to the pickup and dropoff location IDs. We'll use only these two features for our model.

    Turn the dataframe into a list of dictionaries (remember to re-cast the ids to strings - otherwise it will label encode them)
    Fit a dictionary vectorizer
    Get a feature matrix from it

What's the dimensionality of this matrix (number of columns)?


#+begin_src python :tangle /tmp/out.py :results none :exports code :eval no
import pandas as pd
# Press the green button in the gutter to run the script.
df = pd.read_parquet("./yellow_tripdata_2023-01.parquet")
print(df.head())
print(df.columns)
df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)
df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)

df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime
df.duration = df.duration.apply(lambda td:td.total_seconds() /60)
print("df.shape", df.shape)

df = df[(df.duration >=1) & (df.duration <=60)]
print("df.shape", df.shape)

categorical = ['PULocationID', 'DOLocationID']
df[categorical] = df[categorical].astype(str)
# df['PU_DO'] = df['PULocationID'] + '_' + df['DOLocationID']
# categorical = ['PU_DO']
numerical = ['trip_distance']
dicts = df[categorical + numerical].to_dict(orient='records')

from sklearn.feature_extraction import DictVectorizer
vec = DictVectorizer()
X_train = vec.fit_transform(dicts)
X_train
#+end_src
<3009173x516 sparse matrix of type '<class 'numpy.float64'>'
	with 9027519 stored elements in Compressed Sparse Row format>

** 5) Training a model

Now let's use the feature matrix from the previous step to train a model.

    Train a plain linear regression model with default parameters
    Calculate the RMSE of the model on the training data

What's the RMSE on train?

#+begin_src python :tangle /tmp/out.py :results none :exports code :eval no
target = 'duration'
y_train = df[target].values

from sklearn.linear_model import LinearRegression
lr = LinearRegression()

lr = lr.fit(X_train, y_train)
y_pred = lr.predict(X_train)

# sns.distplot(y_pred, label='prediction')
# sns.distplot(y_train, label='actual')
# plt.legend()

from sklearn.metrics import mean_squared_error

mean_squared_error(y_train, y_pred, squared=False)

#+end_src

7.658396898909143

** 6) Evaluating the model

Now let's apply this model to the validation dataset (February 2023).

What's the RMSE on validation?

#+begin_src python :tangle /tmp/out.py :results none :exports code :eval no
df = pd.read_parquet("./yellow_tripdata_2023-02.parquet")

df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)
df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)

df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime
df.duration = df.duration.apply(lambda td:td.total_seconds() /60)
print("df.shape", df.shape)

df = df[(df.duration >=1) & (df.duration <=60)]
print("df.shape", df.shape)

categorical = ['PULocationID', 'DOLocationID']
df[categorical] = df[categorical].astype(str)
# df['PU_DO'] = df['PULocationID'] + '_' + df['DOLocationID']
# categorical = ['PU_DO']
numerical = ['trip_distance']
dicts = df[categorical + numerical].to_dict(orient='records')

from sklearn.feature_extraction import DictVectorizer
vec = DictVectorizer()
X_test = vec.fit_transform(dicts)

target = 'duration'
y_test = df[target].values

y_pred = lr.predict(X_test)
print(mean_squared_error(y_test, y_pred, squared=False))
#+end_src

7.820263388747155
* 2: Experiment Tracking
terms:
- ML experiment ::
- run :: trial in a ML experiment
- run artifact :: any file of ML experiment
- experiment metadata :: info

Experiment Tracking For:
- reproducibility
- organization - automation, collaboration, monitoring
- optimization
- collaboration - visibility of DS efforts

** ml lifecycle
#+ATTR_HTML: :width 700px
[[file:./2-mlflow-exerims/imgs/ml-lifecycle.png]]

#+ATTR_HTML: :width 700px
[[https://i0.wp.com/neptune.ai/wp-content/uploads/2023/10/Experiment-tracking.png?resize=1020%2C534&ssl=1]]
** links
Slides: [[file:./2-mlflow-exerims/Experiment Tracking - MLOps Zoomcamp.pdf]]

* Homework 2: Experiment Tracking
links
- https://courses.datatalks.club/mlops-zoomcamp-2024/homework/hw2
- https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/cohorts/2024/02-experiment-tracking/homework.md

Code: [[file:./2-mlflow-exerims/homework]]
** 1) Install MLflow
: /Volumes/vol2/proj-py/venv/bin/mlflow --version
: mlflow, version 2.13.0
** 2) Download and preprocess the data
Click - to build CLI scripts
- https://pypi.org/project/click/
- https://github.com/pallets/click
- https://click.palletsprojects.com/en/8.1.x/

python preprocess_data.py --raw_data_path <TAXI_DATA_FOLDER> --dest_path ./output

[[file:/ssh:ma:2/preprocess_data.py]]
#+begin_src bash :results output :exports both :dir /ssh:ma:
cd 2/
source /Volumes/vol2/proj-py/venv/bin/activate
/Volumes/vol2/proj-py/venv/bin/python preprocess_data.py
#+end_src

#+RESULTS:

4
** 3) Train a model with autolog

train.py

min_samples_split parameter?
2

[[file:/ssh:ma:2/train.py]]
Install:
#+begin_src bash :results output :exports both :dir /ssh:ma:
: /Volumes/vol2/proxychains-ng-master/proxychains4 /Volumes/vol2/proj-py/venv/bin/python -m pip install mlflow --prefix=/Volumes/vol2/proj-py/venv
#+end_src

#+begin_src elisp :results output :exports both
(python-repl-remote "ma" "/Volumes/vol2/proj-py/venv")
#+end_src

#+begin_src bash :results output :exports both :dir /ssh:ma:
cd 2/
source /Volumes/vol2/proj-py/venv/bin/activate
/Volumes/vol2/proj-py/venv/bin/python train.py
#+end_src

in *train.py* code:
: import mlflow
: mlflow.autolog()

** 4) Launch the tracking server locally
- launch the tracking server on your local machine
- select a SQLite db for the backend store and a folder called artifacts for the artifacts store.

[[file:/ssh:ma:2/train.py]]

: export MLFLOW_TRACKING_URI=sqlite:///mlruns.db
: mlflow ui --port 5000 --backend-store-uri $MLFLOW_TRACKING_URI --default-artifact-root "./artifacts"

in *train.py* code:
: import mlflow
: mlflow.set_tracking_uri("http://localhost:5000")
: mlflow.autolog()

#+begin_src bash :results output :exports both :dir /ssh:ma:
cd 2/
source /Volumes/vol2/proj-py/venv/bin/activate
/Volumes/vol2/proj-py/venv/bin/python train.py
#+end_src


default-artifact-root
** 5) Tune model hyperparameters
make sure that the validation RMSE is logged to the tracking server for each run of the hyperparameter
 optimization optimization (you will need to add a few lines of code to the *objective* function)

hpo.py
[[file:/ssh:ma:2/hpo.py]]

pip install hyperopt

#+begin_src bash :results output :exports both :dir /ssh:ma:
: /Volumes/vol2/proxychains-ng-master/proxychains4 /Volumes/vol2/proj-py/venv/bin/python -m pip install hyperopt --prefix=/Volumes/vol2/proj-py/venv
#+end_src

in *hpo.py* code:
#+begin_src python :results none :exports code :eval no
mlflow.set_tracking_uri("http://127.0.0.1:5000")
mlflow.set_experiment("random-forest-hyperopt")
mlflow.autolog()
with mlflow.start_run(nested=True):
    mlflow.log_param("p", params)
    mlflow.log_metric(key="mean_squared_error", value=rmse)
#+end_src

Run
#+begin_src bash :results output :exports both :dir /ssh:ma:
cd 2/
source /Volumes/vol2/proj-py/venv/bin/activate
/Volumes/vol2/proj-py/venv/bin/python hpo.py
#+end_src


What's the best validation RMSE that you got?
- 5.335
** 6) Promote the best model to the model registry
register_model.py [[file:/ssh:ma:2/register_model.py]]

search_runs from the MlflowClient to get the model with the lowest RMSE,

mlflow.register_model and you will need to pass the right model_uri in the form of a string that looks like this: "runs:/<RUN_ID>/model", and the name of the model (make sure to choose a good one!).

https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/02-experiment-tracking/model-registry.ipynb

register_model.py:
#+begin_src python :results none :exports code :eval no
order_by=["metrics.mean_squared_error ASC"]

train_and_log_model(data_path=data_path, params=run.data.params['p'])

best_run = MlflowClient().search_runs(
    experiment_ids=experiment.experiment_id,
    run_view_type=ViewType.ACTIVE_ONLY,
    max_results=1,
    order_by=["metrics.test_rmse ASC"],
)[0]

print(f"run id: {best_run.info.run_id}, rmse: {best_run.data.metrics['rmse']:.4f}")
#+end_src

#+begin_src bash :results output :exports both :dir /ssh:ma:
cd 2/
source /Volumes/vol2/proj-py/venv/bin/activate
/Volumes/vol2/proj-py/venv/bin/python register_model.py
#+end_src

5.567
* 3: Orchestration
https://github.com/DataTalksClub/mlops-zoomcamp/tree/main/03-orchestration

Code https://github.com/mage-ai/mlops
- docker-compose with mage
- mlflow https://github.com/mage-ai/mlops/pull/6/files
- fork with Mlflow https://github.com/osareniho-oni/mlops-zoomcamp-mage

*Operationalizing ML models* - moving them from development to production
1. optimizing performance, ensuring it handles real-world data, and packaging it for integration into existing systems.
2. moving it from development to production, making it accessible to users and applications.
3. Once deployed, models must be continuously monitored for accuracy and reliability, and may need retraining on new data and updates to maintain effectiveness.
4. model must be integrated into existing workflows, applications, and decision-making processes to drive business impact.

** For:
- Productivity :: collaboration - unified environment
- Reliability :: with clean datasets, proper testing, validation, CI/CD practices, monitoring, and governance.
- Reproducibility :: with versioning datasets, code, and models - transparency and auditability
- Time-to-value :: deploy more projects and derive tangible business value and ROI from AI/ML investments

** tools:
- Airflow
- Prefect https://www.prefect.io/ (Much of Prefect's functionality is backed by an API https://app.prefect.cloud/)
- Mage - https://www.mage.ai/

devops:
- Terraform
- Ansible - configure

** best practices (Mage):
- every block in your data pipeline is a standalone file.
- Data validation is written into each block and tested every time a block is run.
- built-in observability, data quality monitoring, and lineage.
- Each block of code has a single responsibility: load data from a source, transform data, or export data anywhere.
- Data is a first class citizen

Scaling:
- Spark
- Handle data intensive transformations with built-in distributed computing (e.g. Dask, Ray)
- Run thousands of pipelines simultaneously and manage transparently through a collaborative UI.
- Execute SQL queries in your data warehouse to process heavy workloads.
- streaming - Kafka

** *Ansible* - /Terraform/
- Type
  - *Configuration management tool*
    - /Orchestration tool/
- Syntax - *YAML* /HCL/
- Language - *Procedural* /Declarative/
- Default approach
  - *Immutable infrastructure*
    - /Mutable infrastructure/
- Cloud support - *All clouds* /All clouds/
- Lifecycle (state) management
  - *Does not support*
    - /Depends on the lifecycle and state management/
- Packaging and templating
  - *Provides complete support*
    - /Provides partial support/
- Capabilities
  - *Provisioning and configuring*
    - Provisioning and configuring
- Agentless *Yes* /Yes/ - *SSH* /Providers API and SSH/
- Masterless *Yes* /Yes/ - “state” information not require server.
- License - *Open Source* /Business Source License (BUSL)/
- Writen in - *Python, shell* /Go/
- Configuration language - *YAML and Jinja templates* /HashiCorp Configuration Language (HCL)/

Orchestration tools ensure that an environment is in its desired state continuously.

Terraform state files - log  information about the resources.
- to compare infrastructure to the code and make any adjustments as necessary.

OpenTofu is an open-source version of Terraform

Configuration files (\*.tf) -> Terraform Core -> State files (\*.tfstate)

Playbook, Inventory -> Ansible Management Node -> SSH to machines
- Inventory:  IP addresses, databases, servers, and other details.
- Playbook: set of Plays - a set of tasks to run on a specific host or group of hosts.

Terraform - more user-fiendly, good scheduling capabilities.

Ansible - better security and ACL functionality.

Ansible bad with logical dependencies, orchestration services, and interconnected applications.

Terraform :
- infrastructure provisioning
- managing cloud resources
- implementing infrastructure changes
- enabling infrastructure as code practices.

Ansible :
- configuration management
- application deployment
- continuous delivery
- server provisioning
- automating repetitive system administration tasks.

Configuration Drift - difference between the desired and actual state of your configuration.
- Ansible relies on idempotent tasks and continuous execution without maintaining a persistent state of the infrastructure.
- Terraform relies on a stored state to detect and manage drift, emphasizing a declarative approach to infrastructure as code.
* TODO Homework 3: Training Pipelines
https://github.com/DataTalksClub/mlops-zoomcamp/tree/main/03-orchestration

- ML Pipeline: introduction
- Prefect
- Turning a notebook into a pipeline
- Kubeflow Pipeline

* 4: Deployment
https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/04-deployment/README.md

Homework: [[file:./4-depl/README_homework.org]]
** 4.1 Three ways of deploying a model
- Batch vs Online
- For online: web services vs streaming
- Serving models in Batch mode
- Web services
- streaming (AWS Kinesis/SQS + AWS Lambda)

Steps:
- Design: collect requirements + choose right solution
- Training mode: experiment tracking, training pipeline
- Operate face: deployment
  - Batch or offline - if prediction is not required immediately (apply model to data regulaterly)
  - Online - always available
    - Web service
    - Streaming - listening for events in the stream - queue + parallel listening models.

DB - > scoring job(model) -> DB Predictions -> Diagrams

** 4.2 Web-services: Deploying models with Flask and Docker
code: https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/04-deployment/web-service

pipenv install scikit-learn==1.0.2 flask --python=3.9

Docker:
#+begin_src dockerfile
FROM python:3.9.7-slim

RUN pip install -U pip
RUN pip install pipenv

WORKDIR /app

COPY [ "Pipfile", "Pipfile.lock", "./" ]

RUN pipenv install --system --deploy

COPY [ "predict.py", "lin_reg.bin", "./" ]

EXPOSE 9696

ENTRYPOINT [ "gunicorn", "--bind=0.0.0.0:9696", "predict:app" ]
#+end_src

: docker build -t ride-duration-prediction-service:v1 .

: docker run -it --rm -p 9696:9696  ride-duration-prediction-service:v1
** 4.3 Web-services: Getting the models from the model registry (MLflow)
code: https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/04-deployment/web-service-mlflow

*** Download from python (local)
#+begin_src python :results output :exports both :session s1
from mlflow.tracking import MlflowClient
MLFLOW_TRACKING_URI = 'http://127.0.0.1:5000'
RUN_ID = 'b4d3bca8aa8e46a6b8257fe4541b1136'

client = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)

path = client.download_artifacts(run_id=RUN_ID, path='dict_vectorizer.bin')

# print
with open(path, 'rb') as f_out:
    dv = pickle.load(f_out)
#+end_src

*** Download from Python (deployed)
#+begin_src python :results none :exports code :eval no


RUN_ID = os.getenv('RUN_ID')
logged_model = f's3://mlflow-models-alexey/1/{RUN_ID}/artifacts/model'
# logged_model = f'runs:/{RUN_ID}/model'
model = mlflow.pyfunc.load_model(logged_model)
#+end_src

*** Download From shell
#+begin_src bash :eval no :exports code :results none
export MLFLOW_TRACKING_URI="http://127.0.0.1:5000"
export MODEL_RUN_ID="6dd459b11b4e48dc862f4e1019d166f6"

mlflow artifacts download \
    --run-id ${MODEL_RUN_ID} \
    --artifact-path model \
    --dst-path .
#+end_src
** 4.4 (Optional) Streaming: Deploying models with Kinesis and Lambda
code: https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/04-deployment/streaming

Amazon Lambda with Amazon Kinesis https://docs.amazonaws.cn/en_us/lambda/latest/dg/with-kinesis-example.html
- Creating the role (AWS)
- Create a Lambda function, test it (AWS service - piece of code)
- Create a Kinesis stream
- Connect the function to the stream
- Send the recordsk


require: *Boto3* is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python, https://pypi.org/project/boto3/
- https://github.com/boto/boto3.git
- uses:
  - aws_access_key_id = YOUR_KEY
  - aws_secret_access_key = YOUR_SECRET
*** model loading
#+begin_src python :results none :exports code :eval no
import mlflow
logged_model = f's3://mlflow-models-alexey/1/{RUN_ID}/artifacts/model'
# logged_model = f'runs:/{RUN_ID}/model'
model = mlflow.pyfunc.load_model(logged_model)
#+end_src
*** sending data
#+begin_src bash :eval no :exports code :results none
KINESIS_STREAM_INPUT=ride_events
aws kinesis put-record \
    --stream-name ${KINESIS_STREAM_INPUT} \
    --partition-key 1 \
    --data "Hello, this is a test."
#+end_src
*** receiving
#+begin_src bash :eval no :exports code :results none
KINESIS_STREAM_OUTPUT='ride_predictions'
SHARD='shardId-000000000000'

SHARD_ITERATOR=$(aws kinesis \
    get-shard-iterator \
        --shard-id ${SHARD} \
        --shard-iterator-type TRIM_HORIZON \
        --stream-name ${KINESIS_STREAM_OUTPUT} \
        --query 'ShardIterator' \
)

RESULT=$(aws kinesis get-records --shard-iterator $SHARD_ITERATOR)

echo ${RESULT} | jq -r '.Records[0].Data' | base64 --decode
#+end_src
** 4.5 Batch: Preparing a scoring script
Code: https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/04-deployment/batch

steps
- Turn the notebook for training a model into a notebook for applying the model
- Turn the notebook into a script
- Clean it and parametrize

tools:
- mlflow - used for model loading
- prefect
** TODO 4.6 MLOps Zoomcamp 4.6 - Batch scoring with Mage
