;-*- mode: Org; fill-column: 110;-*-
* intro

** 1) Download the data for January and February 2023. Read the data for January. How many columns are there?
https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page

requirements.txt:
- pandas==2.2.2
- pyarrow==16.1.0
- fastparquet==2024.2.0

: import pandas as pd
: df = pd.read_parquet("yellow_tripdata_2023-01.parquet")
: print(len(df.columns))
19

** 2) Computing duration

Now let's compute the duration variable. It should contain the duration of a ride in minutes.
What's the standard deviation of the trips duration in January?

#+begin_src python :tangle /tmp/out.py :results none :exports code :eval no
df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)
df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)

df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime
df.duration = df.duration.apply(lambda td:td.total_seconds() /60)
#+end_src

42.59
** 3) Dropping outliers
Next, we need to check the distribution of the duration variable. There are some outliers. Let's remove them and keep only the records where the duration was between 1 and 60 minutes (inclusive).
What fraction of the records left after you dropped the outliers?

#+begin_src python :tangle /tmp/out.py :results none :exports code :eval no
print(df.shape())
df = df[(df.duration >=1) & (df.duration <=60)]
print(df.shape())
#+end_src
3009173/3066766 = 0.98122

98%


** 4) One-hot encoding
Let's apply one-hot encoding to the pickup and dropoff location IDs. We'll use only these two features for our model.

    Turn the dataframe into a list of dictionaries (remember to re-cast the ids to strings - otherwise it will label encode them)
    Fit a dictionary vectorizer
    Get a feature matrix from it

What's the dimensionality of this matrix (number of columns)?


#+begin_src python :tangle /tmp/out.py :results none :exports code :eval no
import pandas as pd
# Press the green button in the gutter to run the script.
df = pd.read_parquet("./yellow_tripdata_2023-01.parquet")
print(df.head())
print(df.columns)
df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)
df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)

df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime
df.duration = df.duration.apply(lambda td:td.total_seconds() /60)
print("df.shape", df.shape)

df = df[(df.duration >=1) & (df.duration <=60)]
print("df.shape", df.shape)

categorical = ['PULocationID', 'DOLocationID']
df[categorical] = df[categorical].astype(str)
# df['PU_DO'] = df['PULocationID'] + '_' + df['DOLocationID']
# categorical = ['PU_DO']
numerical = ['trip_distance']
dicts = df[categorical + numerical].to_dict(orient='records')

from sklearn.feature_extraction import DictVectorizer
vec = DictVectorizer()
X_train = vec.fit_transform(dicts)
X_train
#+end_src
<3009173x516 sparse matrix of type '<class 'numpy.float64'>'
	with 9027519 stored elements in Compressed Sparse Row format>

** 5) Training a model

Now let's use the feature matrix from the previous step to train a model.

    Train a plain linear regression model with default parameters
    Calculate the RMSE of the model on the training data

What's the RMSE on train?

#+begin_src python :tangle /tmp/out.py :results none :exports code :eval no
target = 'duration'
y_train = df[target].values

from sklearn.linear_model import LinearRegression
lr = LinearRegression()

lr = lr.fit(X_train, y_train)
y_pred = lr.predict(X_train)

# sns.distplot(y_pred, label='prediction')
# sns.distplot(y_train, label='actual')
# plt.legend()

from sklearn.metrics import mean_squared_error

mean_squared_error(y_train, y_pred, squared=False)

#+end_src

7.658396898909143

** 6) Evaluating the model

Now let's apply this model to the validation dataset (February 2023).

What's the RMSE on validation?

#+begin_src python :tangle /tmp/out.py :results none :exports code :eval no
df = pd.read_parquet("./yellow_tripdata_2023-02.parquet")

df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)
df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)

df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime
df.duration = df.duration.apply(lambda td:td.total_seconds() /60)
print("df.shape", df.shape)

df = df[(df.duration >=1) & (df.duration <=60)]
print("df.shape", df.shape)

categorical = ['PULocationID', 'DOLocationID']
df[categorical] = df[categorical].astype(str)
# df['PU_DO'] = df['PULocationID'] + '_' + df['DOLocationID']
# categorical = ['PU_DO']
numerical = ['trip_distance']
dicts = df[categorical + numerical].to_dict(orient='records')

from sklearn.feature_extraction import DictVectorizer
vec = DictVectorizer()
X_test = vec.fit_transform(dicts)

target = 'duration'
y_test = df[target].values

y_pred = lr.predict(X_test)
print(mean_squared_error(y_test, y_pred, squared=False))
#+end_src

7.820263388747155
* Homework 2: Experiment Tracking
links
- https://courses.datatalks.club/mlops-zoomcamp-2024/homework/hw2
- https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/cohorts/2024/02-experiment-tracking/homework.md

Code: [[file:./2-mlflow]]
** 1) Install MLflow
: /Volumes/vol2/proj-py/venv/bin/mlflow --version
: mlflow, version 2.13.0
** 2) Download and preprocess the data
Click - to build CLI scripts
- https://pypi.org/project/click/
- https://github.com/pallets/click
- https://click.palletsprojects.com/en/8.1.x/

python preprocess_data.py --raw_data_path <TAXI_DATA_FOLDER> --dest_path ./output

[[file:/ssh:ma:2/preprocess_data.py]]
#+begin_src bash :results output :exports both :dir /ssh:ma:
cd 2/
source /Volumes/vol2/proj-py/venv/bin/activate
/Volumes/vol2/proj-py/venv/bin/python preprocess_data.py
#+end_src

#+RESULTS:

4
** 3) Train a model with autolog

train.py

min_samples_split parameter?
2

[[file:/ssh:ma:2/train.py]]
Install:
#+begin_src bash :results output :exports both :dir /ssh:ma:
: /Volumes/vol2/proxychains-ng-master/proxychains4 /Volumes/vol2/proj-py/venv/bin/python -m pip install mlflow --prefix=/Volumes/vol2/proj-py/venv
#+end_src

#+begin_src elisp :results output :exports both
(python-repl-remote "ma" "/Volumes/vol2/proj-py/venv")
#+end_src

#+begin_src bash :results output :exports both :dir /ssh:ma:
cd 2/
source /Volumes/vol2/proj-py/venv/bin/activate
/Volumes/vol2/proj-py/venv/bin/python train.py
#+end_src

in *train.py* code:
: import mlflow
: mlflow.autolog()

** 4) Launch the tracking server locally
- launch the tracking server on your local machine
- select a SQLite db for the backend store and a folder called artifacts for the artifacts store.

[[file:/ssh:ma:2/train.py]]

: export MLFLOW_TRACKING_URI=sqlite:///mlruns.db
: mlflow ui --port 5000 --backend-store-uri $MLFLOW_TRACKING_URI --default-artifact-root "./artifacts"

in *train.py* code:
: import mlflow
: mlflow.set_tracking_uri("http://localhost:5000")
: mlflow.autolog()

#+begin_src bash :results output :exports both :dir /ssh:ma:
cd 2/
source /Volumes/vol2/proj-py/venv/bin/activate
/Volumes/vol2/proj-py/venv/bin/python train.py
#+end_src


default-artifact-root
** 5) Tune model hyperparameters
make sure that the validation RMSE is logged to the tracking server for each run of the hyperparameter
 optimization optimization (you will need to add a few lines of code to the *objective* function)

hpo.py
[[file:/ssh:ma:2/hpo.py]]

pip install hyperopt

#+begin_src bash :results output :exports both :dir /ssh:ma:
: /Volumes/vol2/proxychains-ng-master/proxychains4 /Volumes/vol2/proj-py/venv/bin/python -m pip install hyperopt --prefix=/Volumes/vol2/proj-py/venv
#+end_src

in *hpo.py* code:
#+begin_src python :results none :exports code :eval no
mlflow.set_tracking_uri("http://127.0.0.1:5000")
mlflow.set_experiment("random-forest-hyperopt")
mlflow.autolog()
with mlflow.start_run(nested=True):
    mlflow.log_param("p", params)
    mlflow.log_metric(key="mean_squared_error", value=rmse)
#+end_src

Run
#+begin_src bash :results output :exports both :dir /ssh:ma:
cd 2/
source /Volumes/vol2/proj-py/venv/bin/activate
/Volumes/vol2/proj-py/venv/bin/python hpo.py
#+end_src


What's the best validation RMSE that you got?
- 5.335
** 6) Promote the best model to the model registry
register_model.py [[file:/ssh:ma:2/register_model.py]]

search_runs from the MlflowClient to get the model with the lowest RMSE,

mlflow.register_model and you will need to pass the right model_uri in the form of a string that looks like this: "runs:/<RUN_ID>/model", and the name of the model (make sure to choose a good one!).

https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/02-experiment-tracking/model-registry.ipynb

register_model.py:
#+begin_src python :results none :exports code :eval no
order_by=["metrics.mean_squared_error ASC"]

train_and_log_model(data_path=data_path, params=run.data.params['p'])

best_run = MlflowClient().search_runs(
    experiment_ids=experiment.experiment_id,
    run_view_type=ViewType.ACTIVE_ONLY,
    max_results=1,
    order_by=["metrics.test_rmse ASC"],
)[0]

print(f"run id: {best_run.info.run_id}, rmse: {best_run.data.metrics['rmse']:.4f}")
#+end_src

#+begin_src bash :results output :exports both :dir /ssh:ma:
cd 2/
source /Volumes/vol2/proj-py/venv/bin/activate
/Volumes/vol2/proj-py/venv/bin/python register_model.py
#+end_src

5.567
